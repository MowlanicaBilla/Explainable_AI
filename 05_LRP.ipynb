{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Use GPU device 0 if available\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU if GPU is not available\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load data\n",
    "TRAIN_ROOT = \"./data/brain_mri_dataset/Training\"\n",
    "TEST_ROOT = \"./data/brain_mri_dataset/Testing\"\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=TRAIN_ROOT)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=TRAIN_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building the model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.vgg16 = models.vgg16(pretrained=True) \n",
    "\n",
    "        # Replace output layer according to our problem\n",
    "        in_feats = self.vgg16.classifier[6].in_features \n",
    "        self.vgg16.classifier[6] = nn.Linear(in_feats, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg16(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mowlanica.billa/Desktop/Desktop/Data_Science/Projects/Explainable_AI/xai_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/mowlanica.billa/Desktop/Desktop/Data_Science/Projects/Explainable_AI/xai_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (vgg16): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNModel()\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prepare data for pretrained model\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=TRAIN_ROOT,\n",
    "        transform=transforms.Compose([\n",
    "                      transforms.Resize((255,255)),\n",
    "                      transforms.ToTensor()\n",
    "        ])\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=TEST_ROOT,\n",
    "        transform=transforms.Compose([\n",
    "                      transforms.Resize((255,255)),\n",
    "                      transforms.ToTensor()\n",
    "        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3655, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3084, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3094, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2885, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4113, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4224, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4437, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2708, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2383, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2283, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3735, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1599, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1128, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2776, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2352, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2408, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1985, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1511, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2644, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1307, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0133, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1605, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1303, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1639, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0100, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0906, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1022, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9889, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1137, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9403, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9388, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0244, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0678, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9283, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9549, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8300, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8813, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0598, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7241, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8077, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7873, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8207, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6983, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7368, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7073, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7357, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5909, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7312, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7681, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6734, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5950, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7877, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5549, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7210, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6277, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4925, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7960, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6626, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6607, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8171, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6253, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5925, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4650, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4961, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6802, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4475, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7079, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5669, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5635, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4219, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5539, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6041, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3013, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5299, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4390, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5910, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7874, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3103, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3038, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4672, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5172, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4834, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4361, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3563, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3897, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5711, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3539, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4680, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3622, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5119, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6541, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6020, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4985, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2015, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4134, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3732, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5146, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3476, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3132, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Labels are automatically one-hot-encoded\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(outputs, labels)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/Desktop/Desktop/Data_Science/Projects/Explainable_AI/xai_env/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop/Data_Science/Projects/Explainable_AI/xai_env/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% Train\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "epochs = 10\n",
    "\n",
    "# Iterate x epochs over the train data\n",
    "for epoch in range(epochs):  \n",
    "    for i, batch in enumerate(train_loader, 0):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # Labels are automatically one-hot-encoded\n",
    "        loss = cross_entropy_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Inspect predictions for first batch\n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.numpy()\n",
    "outputs = model(inputs).max(1).indices.detach().cpu().numpy()\n",
    "comparison = pd.DataFrame()\n",
    "print(\"Batch accuracy: \", (labels==outputs).sum()/len(labels))\n",
    "comparison[\"labels\"] = labels\n",
    "\n",
    "comparison[\"outputs\"] = outputs\n",
    "comparison  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Layerwise relevance propagation for VGG16\n",
    "# For other CNN architectures this code might become more complex\n",
    "# Source: https://git.tu-berlin.de/gmontavon/lrp-tutorial\n",
    "# http://iphome.hhi.de/samek/pdf/MonXAI19.pdf\n",
    "\n",
    "def new_layer(layer, g):\n",
    "    \"\"\"Clone a layer and pass its parameters through the function g.\"\"\"\n",
    "    layer = copy.deepcopy(layer)\n",
    "    try: layer.weight = torch.nn.Parameter(g(layer.weight))\n",
    "    except AttributeError: pass\n",
    "    try: layer.bias = torch.nn.Parameter(g(layer.bias))\n",
    "    except AttributeError: pass\n",
    "    return layer\n",
    "\n",
    "def dense_to_conv(layers):\n",
    "    \"\"\" Converts a dense layer to a conv layer \"\"\"\n",
    "    newlayers = []\n",
    "    for i,layer in enumerate(layers):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            newlayer = None\n",
    "            if i == 0:\n",
    "                m, n = 512, layer.weight.shape[0]\n",
    "                newlayer = nn.Conv2d(m,n,7)\n",
    "                newlayer.weight = nn.Parameter(layer.weight.reshape(n,m,7,7))\n",
    "            else:\n",
    "                m,n = layer.weight.shape[1],layer.weight.shape[0]\n",
    "                newlayer = nn.Conv2d(m,n,1)\n",
    "                newlayer.weight = nn.Parameter(layer.weight.reshape(n,m,1,1))\n",
    "            newlayer.bias = nn.Parameter(layer.bias)\n",
    "            newlayers += [newlayer]\n",
    "        else:\n",
    "            newlayers += [layer]\n",
    "    return newlayers\n",
    "\n",
    "def get_linear_layer_indices(model):\n",
    "    offset = len(model.vgg16._modules['features']) + 1\n",
    "    indices = []\n",
    "    for i, layer in enumerate(model.vgg16._modules['classifier']): \n",
    "        if isinstance(layer, nn.Linear): \n",
    "            indices.append(i)\n",
    "    indices = [offset + val for val in indices]\n",
    "    return indices\n",
    "\n",
    "def apply_lrp_on_vgg16(model, image):\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    # >>> Step 1: Extract layers\n",
    "    layers = list(model.vgg16._modules['features']) \\\n",
    "                + [model.vgg16._modules['avgpool']] \\\n",
    "                + dense_to_conv(list(model.vgg16._modules['classifier']))\n",
    "    linear_layer_indices = get_linear_layer_indices(model)\n",
    "    # >>> Step 2: Propagate image through layers and store activations\n",
    "    n_layers = len(layers)\n",
    "    activations = [image] + [None] * n_layers # list of activations\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        if layer in linear_layer_indices:\n",
    "            if layer == 32:\n",
    "                activations[layer] = activations[layer].reshape((1, 512, 7, 7))\n",
    "        activation = layers[layer].forward(activations[layer])\n",
    "        if isinstance(layers[layer], torch.nn.modules.pooling.AdaptiveAvgPool2d):\n",
    "            activation = torch.flatten(activation, start_dim=1)\n",
    "        activations[layer+1] = activation\n",
    "\n",
    "    # >>> Step 3: Replace last layer with one-hot-encoding\n",
    "    output_activation = activations[-1].detach().cpu().numpy()\n",
    "    max_activation = output_activation.max()\n",
    "    one_hot_output = [val if val == max_activation else 0 \n",
    "                        for val in output_activation[0]]\n",
    "\n",
    "    activations[-1] = torch.FloatTensor([one_hot_output]).to(device)\n",
    "\n",
    "    # >>> Step 4: Backpropagate relevance scores\n",
    "    relevances = [None] * n_layers + [activations[-1]]\n",
    "    # Iterate over the layers in reverse order\n",
    "    for layer in range(0, n_layers)[::-1]:\n",
    "        current = layers[layer]\n",
    "        # Treat max pooling layers as avg pooling\n",
    "        if isinstance(current, torch.nn.MaxPool2d):\n",
    "            layers[layer] = torch.nn.AvgPool2d(2)\n",
    "            current = layers[layer]\n",
    "        if isinstance(current, torch.nn.Conv2d) or \\\n",
    "           isinstance(current, torch.nn.AvgPool2d) or\\\n",
    "           isinstance(current, torch.nn.Linear):\n",
    "            activations[layer] = activations[layer].data.requires_grad_(True)\n",
    "            \n",
    "            # Apply variants of LRP depending on the depth\n",
    "            # see: https://link.springer.com/chapter/10.1007%2F978-3-030-28954-6_10\n",
    "            # Lower layers, LRP-gamma >> Favor positive contributions (activations)\n",
    "            if layer <= 16:       rho = lambda p: p + 0.25*p.clamp(min=0); incr = lambda z: z+1e-9\n",
    "            # Middle layers, LRP-epsilon >> Remove some noise / Only most salient factors survive\n",
    "            if 17 <= layer <= 30: rho = lambda p: p;                       incr = lambda z: z+1e-9+0.25*((z**2).mean()**.5).data\n",
    "            # Upper Layers, LRP-0 >> Basic rule\n",
    "            if layer >= 31:       rho = lambda p: p;                       incr = lambda z: z+1e-9\n",
    "            \n",
    "            # Transform weights of layer and execute forward pass\n",
    "            z = incr(new_layer(layers[layer],rho).forward(activations[layer]))\n",
    "            # Element-wise division between relevance of the next layer and z\n",
    "            s = (relevances[layer+1]/z).data                                     \n",
    "            # Calculate the gradient and multiply it by the activation\n",
    "            (z * s).sum().backward(); \n",
    "            c = activations[layer].grad       \n",
    "            # Assign new relevance values           \n",
    "            relevances[layer] = (activations[layer]*c).data                          \n",
    "        else:\n",
    "            relevances[layer] = relevances[layer+1]\n",
    "\n",
    "    # >>> Potential Step 5: Apply different propagation rule for pixels\n",
    "    return relevances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relevances for first image in this test batch\n",
    "image_id = 31\n",
    "image_relevances = apply_lrp_on_vgg16(model, inputs[image_id])\n",
    "image_relevances = image_relevances.permute(0,2,3,1).detach().cpu().numpy()[0]\n",
    "image_relevances = np.interp(image_relevances, (image_relevances.min(),\n",
    "                                                image_relevances.max()), \n",
    "                                                (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show relevances\n",
    "pred_label = list(test_dataset.class_to_idx.keys())[\n",
    "             list(test_dataset.class_to_idx.values())\n",
    "            .index(labels[image_id])]\n",
    "if outputs[image_id] == labels[image_id]:\n",
    "    print(\"Groundtruth for this image: \", pred_label)\n",
    "\n",
    "    # Plot images next to each other\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image_relevances[:,:,0], cmap=\"seismic\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(inputs[image_id].permute(1,2,0).detach().cpu().numpy())\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"This image is not classified correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
